{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df3727cd-2b1e-440b-8a4a-80313295144d",
   "metadata": {},
   "source": [
    "### Install BELA workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59364c46-1744-4646-b332-9b5ffad2e9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "! git clone https://github.com/PyThaiNLP/MultiEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fd92862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting multiel\n",
      "  Downloading MultiEL-0.5-py3-none-any.whl (4.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hCollecting pytorch-lightning\n",
      "  Downloading pytorch_lightning-2.0.7-py3-none-any.whl (724 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m725.0/725.0 kB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sentencepiece in /usr/local/lib/python3.8/dist-packages (from multiel) (0.1.95)\n",
      "Collecting hydra-submitit-launcher\n",
      "  Downloading hydra_submitit_launcher-1.2.0-py3-none-any.whl (5.2 kB)\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.8/dist-packages (from multiel) (0.8.1)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (from multiel) (4.21.0)\n",
      "Collecting protobuf==3.20\n",
      "  Downloading protobuf-3.20.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting ujson\n",
      "  Downloading ujson-5.8.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from multiel) (5.4.1)\n",
      "Requirement already satisfied: fairscale in /usr/local/lib/python3.8/dist-packages (from multiel) (0.4.9)\n",
      "Requirement already satisfied: hydra-core in /usr/local/lib/python3.8/dist-packages (from multiel) (1.0.7)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from multiel) (4.64.0)\n",
      "Collecting accelerate>=0.9.0\n",
      "  Downloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting h5py\n",
      "  Downloading h5py-3.9.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting faiss-gpu\n",
      "  Downloading faiss_gpu-1.7.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from accelerate>=0.9.0->multiel) (21.3)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from accelerate>=0.9.0->multiel) (5.9.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.8/dist-packages (from accelerate>=0.9.0->multiel) (1.12.1+cu113)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from accelerate>=0.9.0->multiel) (1.22.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub->multiel) (4.3.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub->multiel) (3.7.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from huggingface-hub->multiel) (2.28.1)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.8 in /usr/local/lib/python3.8/dist-packages (from hydra-core->multiel) (4.8)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.8/dist-packages (from hydra-core->multiel) (5.9.0)\n",
      "Requirement already satisfied: omegaconf<2.1,>=2.0.5 in /usr/local/lib/python3.8/dist-packages (from hydra-core->multiel) (2.0.6)\n",
      "Collecting submitit>=1.3.3\n",
      "  Downloading submitit-1.4.5-py3-none-any.whl (73 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.1/73.1 kB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting hydra-core\n",
      "  Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading hydra_core-1.3.1-py3-none-any.whl (154 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.1/154.1 kB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading hydra_core-1.3.0-py3-none-any.whl (153 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.8/153.8 kB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading hydra_core-1.3.0.dev1-py3-none-any.whl (153 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.7/153.7 kB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading hydra_core-1.3.0.dev0-py3-none-any.whl (151 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.4/151.4 kB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading hydra_core-1.2.0-py3-none-any.whl (151 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.1/151.1 kB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading hydra_core-1.2.0.dev5-py3-none-any.whl (150 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.5/150.5 kB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading hydra_core-1.2.0.dev4-py3-none-any.whl (150 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.5/150.5 kB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading hydra_core-1.2.0.dev3-py3-none-any.whl (150 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.3/150.3 kB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading hydra_core-1.2.0.dev2-py3-none-any.whl (146 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.9/146.9 kB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading hydra_core-1.2.0.dev1-py3-none-any.whl (146 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.9/146.9 kB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading hydra_core-1.1.2-py3-none-any.whl (147 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.4/147.4 kB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting importlib-resources\n",
      "  Downloading importlib_resources-5.2.3-py3-none-any.whl (27 kB)\n",
      "Collecting hydra-core\n",
      "  Downloading hydra_core-1.1.2.dev0-py3-none-any.whl (146 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.4/146.4 kB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading hydra_core-1.1.1-py3-none-any.whl (145 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.8/145.8 kB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading hydra_core-1.1.0-py3-none-any.whl (144 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.6/144.6 kB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading hydra_core-1.1.0rc1-py3-none-any.whl (144 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.6/144.6 kB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading hydra_core-1.1.0.dev7-py3-none-any.whl (144 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.7/144.7 kB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of hydra-submitit-launcher to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting hydra-submitit-launcher\n",
      "  Downloading hydra_submitit_launcher-1.1.6-py3-none-any.whl (5.2 kB)\n",
      "  Downloading hydra_submitit_launcher-1.1.5-py3-none-any.whl (5.1 kB)\n",
      "  Downloading hydra_submitit_launcher-1.1.1-py3-none-any.whl (5.1 kB)\n",
      "Collecting lightning-utilities>=0.7.0\n",
      "  Downloading lightning_utilities-0.9.0-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning->multiel) (2022.5.0)\n",
      "Collecting torchmetrics>=0.7.0\n",
      "  Downloading torchmetrics-1.0.3-py3-none-any.whl (731 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.6/731.6 kB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers->multiel) (2022.7.25)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers->multiel) (0.12.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning->multiel) (3.8.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->accelerate>=0.9.0->multiel) (3.0.9)\n",
      "Requirement already satisfied: cloudpickle>=1.2.1 in /usr/local/lib/python3.8/dist-packages (from submitit>=1.3.3->hydra-submitit-launcher->multiel) (2.2.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from importlib-resources->hydra-core->multiel) (3.8.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub->multiel) (2022.6.15)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub->multiel) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub->multiel) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub->multiel) (1.26.11)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->fsspec[http]>2021.06.0->pytorch-lightning->multiel) (1.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->fsspec[http]>2021.06.0->pytorch-lightning->multiel) (1.7.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->fsspec[http]>2021.06.0->pytorch-lightning->multiel) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->fsspec[http]>2021.06.0->pytorch-lightning->multiel) (6.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->fsspec[http]>2021.06.0->pytorch-lightning->multiel) (21.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->fsspec[http]>2021.06.0->pytorch-lightning->multiel) (1.2.0)\n",
      "Installing collected packages: faiss-gpu, ujson, submitit, protobuf, h5py, lightning-utilities, accelerate, torchmetrics, hydra-submitit-launcher, pytorch-lightning, multiel\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.19.5\n",
      "    Uninstalling protobuf-3.19.5:\n",
      "      Successfully uninstalled protobuf-3.19.5\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorboard 2.10.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.0 which is incompatible.\n",
      "detectron2 0.6 requires hydra-core>=1.1, but you have hydra-core 1.0.7 which is incompatible.\n",
      "detectron2 0.6 requires omegaconf>=2.1, but you have omegaconf 2.0.6 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed accelerate-0.21.0 faiss-gpu-1.7.2 h5py-3.9.0 hydra-submitit-launcher-1.1.1 lightning-utilities-0.9.0 multiel-0.5 protobuf-3.20.0 pytorch-lightning-2.0.7 submitit-1.4.5 torchmetrics-1.0.3 ujson-5.8.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install multiel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31b8aaf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from multiel import BELA\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b751d626",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set intial threshold for NER and EL to 0.01 to increase amount of candidates\n",
    "bela_run = BELA(\n",
    " md_threshold=0.01,\n",
    " el_threshold=0.01, \n",
    " checkpoint_name =\"wiki\", \n",
    " device = \"cuda:0\",\n",
    " config_name=\"joint_el_mel_new\",\n",
    " repo =\"wannaphong/BELA\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c56b23",
   "metadata": {},
   "source": [
    "### WDSQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "80c50e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wdsq = pd.read_csv('workspace/kbqa/kbqa/thesis/Results_analysis/wdsq_res_init.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b061f3-976e-4435-ba18-e6cf2158b3b4",
   "metadata": {},
   "source": [
    "### 4 modes:\n",
    "1) 'bela_base' -- take initial question and rub BELA pipeline\n",
    "2) 'bela_large_case' -- take initial question, capitalize all words in the question and run BELA pipeline\n",
    "3) 'bela_large_ner' -- take initial question, capitalize just NER (use the code below) and run BELA pipeline\n",
    "4) 'bela_just_ner' -- take just NER from question (use the code below) and run BELA pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ba31a8b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5622/5622 [03:21<00:00, 27.96it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(df_wdsq))):\n",
    "    df_wdsq.loc[i, 'bela_just_ner'] = bela_run.process_batch([df_wdsq.loc[i, 'ner_largecase']])\n",
    "#     df_wdsq.loc[i, 'bela_large_ner'] = bela_run.process_batch([df_wdsq.loc[i, 'ques_ner_largecase']])\n",
    "#     ques = ' '.join([elem.capitalize() for elem in df_wdsq.loc[i, 'question'].split(' ')])\n",
    "#     df_wdsq.loc[i, 'bela_large_case'] = bela_run.process_batch([ques])\n",
    "#     df_wdsq.loc[i, 'bela_base'] = bela_run.process_batch([df_wdsq.loc[i, 'question']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a5371982",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5622/5622 [00:00<00:00, 7015.72it/s]\n"
     ]
    }
   ],
   "source": [
    "#rerank bela entities by el scores\n",
    "for k in tqdm(range(len(df_wdsq))):\n",
    "    inds = np.argsort(df_wdsq.loc[k, 'bela_just_ner'][0]['el_scores'])[::-1]\n",
    "    new_arr =[df_wdsq.loc[k, 'bela_just_ner'][0]['entities'][i] for i in inds]\n",
    "    cur_set = set()\n",
    "    fin_ents = []\n",
    "    for j in new_arr:\n",
    "        if j not in cur_set:\n",
    "            cur_set.add(j)\n",
    "            fin_ents.append(j)\n",
    "    df_wdsq.loc[k, 'bela_just_ner_ents'] = ', '.join(fin_ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "38caecdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_wdsq.query(\"bela_large_ner_ents == ''\"))/len(df_wdsq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "aa54194a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "528\n",
      "3291\n",
      "5268\n",
      "5519\n"
     ]
    }
   ],
   "source": [
    "#if empty, take another prediction\n",
    "for i in range(len(df_wdsq)):\n",
    "    if df_wdsq.loc[i, \"bela_large_case_ents\"] == '':\n",
    "        print(i)\n",
    "        df_wdsq.loc[i, 'bela_large_case'] = df_wdsq.loc[i, 'bela_base']\n",
    "        df_wdsq.loc[i, 'bela_large_case_ents'] = df_wdsq.loc[i, 'bela_base_ents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c95f572e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5622/5622 [00:01<00:00, 4588.45it/s]\n"
     ]
    }
   ],
   "source": [
    "#use 'bela_large_case' to retrieve 'ques_ner_largecase' and 'ner_largecase'\n",
    "\n",
    "for i in tqdm(range(len(df_wdsq))):\n",
    "    best_ent = np.argsort(df_wdsq.loc[i, 'bela_large_case'][0]['el_scores'])[::-1][0]\n",
    "    start = df_wdsq.loc[i, 'bela_large_case'][0]['offsets'][best_ent]\n",
    "    length = df_wdsq.loc[i, 'bela_large_case'][0]['lengths'][best_ent]\n",
    "    part = ' '.join([elem.capitalize() for elem in df_wdsq.loc[i, 'question'][start:start+length].split(' ')])\n",
    "    ques = df_wdsq.loc[i, 'question'][:start]+part+df_wdsq.loc[i, 'question'][start+length:]\n",
    "    df_wdsq.loc[i, 'ques_ner_largecase'] = ques\n",
    "    df_wdsq.loc[i, 'ner_largecase'] = part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "b185a1a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5622/5622 [00:00<00:00, 6049.48it/s]\n"
     ]
    }
   ],
   "source": [
    "#prepare questions with BELA NER for mGENRE\n",
    "\n",
    "for i in tqdm(range(len(df_wdsq))):\n",
    "    best_ent = np.argsort(df_wdsq.loc[i, 'bela_large_case'][0]['el_scores'])[::-1][0]\n",
    "    start = df_wdsq.loc[i, 'bela_large_case'][0]['offsets'][best_ent]\n",
    "    length = df_wdsq.loc[i, 'bela_large_case'][0]['lengths'][best_ent]\n",
    "    part = ' '.join([elem.capitalize() for elem in df_wdsq.loc[i, 'question'][start:start+length].split(' ')])\n",
    "    begin = ' '.join([elem.capitalize() for elem in df_wdsq.loc[i, 'question'][:start].split(' ')])\n",
    "    end = ' '.join([elem.capitalize() for elem in df_wdsq.loc[i, 'question'][start+length:].split(' ')])\n",
    "    ques = begin+'[START] '+part+' [END]'+end\n",
    "    df_wdsq.loc[i, 'mgenre_largecase_all'] = ques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fca21473",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topk_accuracy(df, col):\n",
    "    \n",
    "    count_1 = 0\n",
    "    count_2 = 0\n",
    "    count_3 = 0\n",
    "    count_4 = 0\n",
    "    count_5 = 0\n",
    "    count_6 = 0\n",
    "    for k in range(len(df)):\n",
    "        \n",
    "        if df.loc[k, col].split(', ')[0] == df.loc[k, 'subject']:\n",
    "            count_1 += 1\n",
    "            count_2 += 1\n",
    "            count_3 += 1\n",
    "            count_4 += 1\n",
    "            count_5 += 1\n",
    "            count_6 += 1\n",
    "        elif len(df.loc[k, col].split(', ')) >= 2 and df.loc[k, col].split(', ')[1] == df.loc[k, 'subject']:\n",
    "            count_2 += 1\n",
    "            count_3 += 1\n",
    "            count_4 += 1\n",
    "            count_5 += 1\n",
    "            count_6 += 1\n",
    "        elif len(df.loc[k, col].split(', ')) >= 3 and df.loc[k, col].split(', ')[2] == df.loc[k, 'subject']:\n",
    "            count_3 += 1\n",
    "            count_4 += 1\n",
    "            count_5 += 1\n",
    "            count_6 += 1   \n",
    "        elif len(df.loc[k, col].split(', ')) >= 4 and df.loc[k, col].split(', ')[3] == df.loc[k, 'subject']:\n",
    "            count_4 += 1\n",
    "            count_5 += 1\n",
    "            count_6 += 1   \n",
    "        elif len(df.loc[k, col].split(', ')) >= 5 and df.loc[k, col].split(', ')[4] == df.loc[k, 'subject']:\n",
    "            count_5 += 1\n",
    "            count_6 += 1\n",
    "        elif len(df.loc[k, col].split(', ')) >= 6 and df.loc[k, col].split(', ')[5] == df.loc[k, 'subject']:\n",
    "            count_6 += 1\n",
    "                \n",
    "    print('Top-1 accuracy:', count_1/len(df))\n",
    "    print('Top-2 accuracy:', count_2/len(df))\n",
    "    print('Top-3 accuracy:', count_3/len(df))\n",
    "    print('Top-4 accuracy:', count_4/len(df))\n",
    "    print('Top-5 accuracy:', count_5/len(df))\n",
    "    print('Top-6 accuracy:', count_6/len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c8514b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 accuracy: 0.5597652081109925\n",
      "Top-2 accuracy: 0.6561721807186055\n",
      "Top-3 accuracy: 0.6860547847741018\n",
      "Top-4 accuracy: 0.6997509782995376\n",
      "Top-5 accuracy: 0.7072216293134116\n",
      "Top-6 accuracy: 0.7127356812522234\n"
     ]
    }
   ],
   "source": [
    "topk_accuracy(df_wdsq, 'bela_base_ents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "69e191d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 accuracy: 0.6463891853432941\n",
      "Top-2 accuracy: 0.7394165777303451\n",
      "Top-3 accuracy: 0.7689434364994664\n",
      "Top-4 accuracy: 0.7801494130202775\n",
      "Top-5 accuracy: 0.7858413376022768\n",
      "Top-6 accuracy: 0.7879758093205265\n"
     ]
    }
   ],
   "source": [
    "topk_accuracy(df_wdsq, 'bela_large_case_ents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "dbbccc9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 accuracy: 0.6604411241551049\n",
      "Top-2 accuracy: 0.7427961579509071\n",
      "Top-3 accuracy: 0.7696549270722163\n",
      "Top-4 accuracy: 0.7822838847385272\n",
      "Top-5 accuracy: 0.7881536819637139\n",
      "Top-6 accuracy: 0.7911775168979011\n"
     ]
    }
   ],
   "source": [
    "topk_accuracy(df_wdsq, 'bela_large_ner_ents')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "a580e01f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 accuracy: 0.5962290999644255\n",
      "Top-2 accuracy: 0.6536819637139808\n",
      "Top-3 accuracy: 0.6673781572394166\n",
      "Top-4 accuracy: 0.6725364638918534\n",
      "Top-5 accuracy: 0.675382426182853\n",
      "Top-6 accuracy: 0.6768054073283529\n"
     ]
    }
   ],
   "source": [
    "topk_accuracy(df_wdsq, 'bela_just_ner_ents')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "0b378801",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wdsq.to_csv('workspace/kbqa/kbqa/thesis/ent_linking_res/bela_for_mgenre_ques.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23a37eb",
   "metadata": {},
   "source": [
    "### RuBQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "c3f6d483",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rubq = pd.read_csv('workspace/kbqa/kbqa/thesis/ent_linking_res/rubq_for_bela.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba15b2ba-c2e5-456b-8a89-96c9fb31fff2",
   "metadata": {},
   "source": [
    "### swith between russian and enligsh translation of the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "cbaba7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1754/1754 [01:00<00:00, 28.80it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(df_rubq))):\n",
    "    df_rubq.loc[i, 'bela_just_ner'] = bela_run.process_batch([df_rubq.loc[i, 'ner_largecase']])\n",
    "#     df_rubq.loc[i, 'bela_large_ner'] = bela_run.process_batch([df_rubq.loc[i, 'ques_ner_largecase']])\n",
    "#     ques = ' '.join([elem.capitalize() for elem in df_rubq.loc[i, 'question_ru'].split(' ')])\n",
    "#     df_rubq.loc[i, 'bela_large_case_ru'] = bela_run.process_batch([ques])\n",
    "#     df_rubq.loc[i, 'bela_base_ru'] = bela_run.process_batch([df_rubq.loc[i, 'question_ru']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "29bea54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1754/1754 [00:00<00:00, 8013.36it/s]\n"
     ]
    }
   ],
   "source": [
    "for k in tqdm(range(len(df_rubq))):\n",
    "    inds = np.argsort(df_rubq.loc[k, 'bela_just_ner'][0]['el_scores'])[::-1]\n",
    "    new_arr =[df_rubq.loc[k, 'bela_just_ner'][0]['entities'][i] for i in inds]\n",
    "    cur_set = set()\n",
    "    fin_ents = []\n",
    "    for j in new_arr:\n",
    "        if j not in cur_set:\n",
    "            cur_set.add(j)\n",
    "            fin_ents.append(j)\n",
    "    df_rubq.loc[k, 'bela_just_ner_ents'] = ', '.join(fin_ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "7388aece",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1754/1754 [00:00<00:00, 4006.32it/s]\n"
     ]
    }
   ],
   "source": [
    "#retrieve question with capitalized ner and ner\n",
    "\n",
    "for i in tqdm(range(len(df_rubq))):\n",
    "    best_ent = np.argsort(df_rubq.loc[i, 'bela_base'][0]['el_scores'])[::-1][0]\n",
    "    start = df_rubq.loc[i, 'bela_base'][0]['offsets'][best_ent]\n",
    "    length = df_rubq.loc[i, 'bela_base'][0]['lengths'][best_ent]\n",
    "    part = ' '.join([elem.capitalize() for elem in df_rubq.loc[i, 'question'][start:start+length].split(' ')])\n",
    "    ques = df_rubq.loc[i, 'question'][:start]+part+df_rubq.loc[i, 'question'][start+length:]\n",
    "    df_rubq.loc[i, 'ques_ner_largecase'] = ques\n",
    "    df_rubq.loc[i, 'ner_largecase'] = part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b884e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_rubq.query(\"ner_largecase_ru == ''\"))/len(df_rubq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "bda6b1ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1101\n"
     ]
    }
   ],
   "source": [
    "#if empty, take another prediction\n",
    "for i in range(len(df_rubq)):\n",
    "    if df_rubq.loc[i, \"bela_large_case_ents\"] == '':\n",
    "        print(i)\n",
    "        df_rubq.loc[i, 'bela_large_case'] = df_rubq.loc[i, 'bela_base']\n",
    "        df_rubq.loc[i, 'bela_large_case_ents'] = df_rubq.loc[i, 'bela_base_ents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "9a01c82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 accuracy: 0.6824401368301026\n",
      "Top-2 accuracy: 0.7639680729760547\n",
      "Top-3 accuracy: 0.7833523375142531\n",
      "Top-4 accuracy: 0.7924743443557583\n",
      "Top-5 accuracy: 0.7953249714937286\n",
      "Top-6 accuracy: 0.7970353477765109\n"
     ]
    }
   ],
   "source": [
    "topk_accuracy(df_rubq, 'bela_base_ents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "efb93e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 accuracy: 0.6322690992018244\n",
      "Top-2 accuracy: 0.7246294184720639\n",
      "Top-3 accuracy: 0.7485746864310148\n",
      "Top-4 accuracy: 0.758266818700114\n",
      "Top-5 accuracy: 0.7605473204104903\n",
      "Top-6 accuracy: 0.7622576966932725\n"
     ]
    }
   ],
   "source": [
    "topk_accuracy(df_rubq, 'bela_large_case_ents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "24c2a2f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 accuracy: 0.6727480045610034\n",
      "Top-2 accuracy: 0.749714937286203\n",
      "Top-3 accuracy: 0.7730900798175598\n",
      "Top-4 accuracy: 0.7839224629418472\n",
      "Top-5 accuracy: 0.7873432155074116\n",
      "Top-6 accuracy: 0.7884834663625998\n"
     ]
    }
   ],
   "source": [
    "topk_accuracy(df_rubq, 'bela_large_ner_ents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "773f0115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 accuracy: 0.5809578107183581\n",
      "Top-2 accuracy: 0.604903078677309\n",
      "Top-3 accuracy: 0.6077537058152793\n",
      "Top-4 accuracy: 0.6094640820980616\n",
      "Top-5 accuracy: 0.6100342075256556\n",
      "Top-6 accuracy: 0.6100342075256556\n"
     ]
    }
   ],
   "source": [
    "topk_accuracy(df_rubq, 'bela_just_ner_ents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "6514539e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 accuracy: 0.7696693272519954\n",
      "Top-2 accuracy: 0.838654503990878\n",
      "Top-3 accuracy: 0.8580387685290763\n",
      "Top-4 accuracy: 0.8700114025085519\n",
      "Top-5 accuracy: 0.8728620296465223\n",
      "Top-6 accuracy: 0.8779931584948689\n"
     ]
    }
   ],
   "source": [
    "topk_accuracy(df_rubq, 'bela_base_ru_ents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "c562bff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 accuracy: 0.7183580387685291\n",
      "Top-2 accuracy: 0.8055872291904219\n",
      "Top-3 accuracy: 0.8232611174458381\n",
      "Top-4 accuracy: 0.8352337514253135\n",
      "Top-5 accuracy: 0.8432155074116305\n",
      "Top-6 accuracy: 0.8449258836944128\n"
     ]
    }
   ],
   "source": [
    "topk_accuracy(df_rubq, 'bela_large_case_ru_ents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "b854fdcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 accuracy: 0.75769669327252\n",
      "Top-2 accuracy: 0.8204104903078677\n",
      "Top-3 accuracy: 0.8426453819840365\n",
      "Top-4 accuracy: 0.8557582668187002\n",
      "Top-5 accuracy: 0.8597491448118586\n",
      "Top-6 accuracy: 0.863740022805017\n"
     ]
    }
   ],
   "source": [
    "topk_accuracy(df_rubq, 'bela_large_ner_ru_ents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "564b7375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 accuracy: 0.6237172177879133\n",
      "Top-2 accuracy: 0.645381984036488\n",
      "Top-3 accuracy: 0.6510832383124288\n",
      "Top-4 accuracy: 0.6539338654503991\n",
      "Top-5 accuracy: 0.6545039908779932\n",
      "Top-6 accuracy: 0.6556442417331813\n"
     ]
    }
   ],
   "source": [
    "topk_accuracy(df_rubq, 'bela_just_ner_ru_ents')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e287ab",
   "metadata": {},
   "source": [
    "### Russian+English"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c0b5b3-b182-4a1d-bba2-f429478a409f",
   "metadata": {},
   "source": [
    "#### join Russian and English predictions for \"bela_base mode\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "06d0e2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1754/1754 [00:00<00:00, 5752.66it/s]\n"
     ]
    }
   ],
   "source": [
    "for k in tqdm(range(len(df_rubq))):\n",
    "    \n",
    "    inds1 = df_rubq.loc[k, 'bela_base'][0]['el_scores']\n",
    "    inds2 = df_rubq.loc[k, 'bela_base_ru'][0]['el_scores']\n",
    "    inds = np.argsort(inds1+inds2)[::-1]\n",
    "\n",
    "    ents = df_rubq.loc[k, 'bela_base'][0]['entities']+df_rubq.loc[k, 'bela_base_ru'][0]['entities']\n",
    "\n",
    "    new_arr =[ents[i] for i in inds]\n",
    "\n",
    "    cur_set = set()\n",
    "    fin_ents = []\n",
    "    for j in new_arr:\n",
    "        if j not in cur_set:\n",
    "            cur_set.add(j)\n",
    "            fin_ents.append(j)\n",
    "    df_rubq.loc[k, 'bela_base_joined'] = ', '.join(fin_ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67d543f-3854-4a78-a688-4209371a3b1a",
   "metadata": {},
   "source": [
    "#### join Russian and English predictions for \"bela_large_case mode\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "a9c7df65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1754/1754 [00:00<00:00, 5775.74it/s]\n"
     ]
    }
   ],
   "source": [
    "for k in tqdm(range(len(df_rubq))):\n",
    "    \n",
    "    inds1 = df_rubq.loc[k, 'bela_large_case'][0]['el_scores']\n",
    "    inds2 = df_rubq.loc[k, 'bela_large_case_ru'][0]['el_scores']\n",
    "    inds = np.argsort(inds1+inds2)[::-1]\n",
    "\n",
    "    ents = df_rubq.loc[k, 'bela_large_case'][0]['entities']+df_rubq.loc[k, 'bela_large_case_ru'][0]['entities']\n",
    "\n",
    "    new_arr =[ents[i] for i in inds]\n",
    "\n",
    "    cur_set = set()\n",
    "    fin_ents = []\n",
    "    for j in new_arr:\n",
    "        if j not in cur_set:\n",
    "            cur_set.add(j)\n",
    "            fin_ents.append(j)\n",
    "    df_rubq.loc[k, 'bela_large_case_joined'] = ', '.join(fin_ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bcbbcb-6821-4af7-8d73-7bf82fd45508",
   "metadata": {},
   "source": [
    "#### join Russian and English predictions for \"bela_large_ner mode\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "f676d469",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1754/1754 [00:00<00:00, 5928.38it/s]\n"
     ]
    }
   ],
   "source": [
    "for k in tqdm(range(len(df_rubq))):\n",
    "    \n",
    "    inds1 = df_rubq.loc[k, 'bela_large_ner'][0]['el_scores']\n",
    "    inds2 = df_rubq.loc[k, 'bela_large_ner_ru'][0]['el_scores']\n",
    "    inds = np.argsort(inds1+inds2)[::-1]\n",
    "\n",
    "    ents = df_rubq.loc[k, 'bela_large_ner'][0]['entities']+df_rubq.loc[k, 'bela_large_ner_ru'][0]['entities']\n",
    "\n",
    "    new_arr =[ents[i] for i in inds]\n",
    "\n",
    "    cur_set = set()\n",
    "    fin_ents = []\n",
    "    for j in new_arr:\n",
    "        if j not in cur_set:\n",
    "            cur_set.add(j)\n",
    "            fin_ents.append(j)\n",
    "    df_rubq.loc[k, 'bela_large_ner_joined'] = ', '.join(fin_ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0629b00-cb90-4989-b368-aabc2ebdc3c6",
   "metadata": {},
   "source": [
    "#### join Russian and English predictions for \"bela_just_ner mode\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "9ac96023",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1754/1754 [00:00<00:00, 7994.93it/s]\n"
     ]
    }
   ],
   "source": [
    "for k in tqdm(range(len(df_rubq))):\n",
    "    \n",
    "    inds1 = df_rubq.loc[k, 'bela_just_ner'][0]['el_scores']\n",
    "    inds2 = df_rubq.loc[k, 'bela_just_ner_ru'][0]['el_scores']\n",
    "    inds = np.argsort(inds1+inds2)[::-1]\n",
    "\n",
    "    ents = df_rubq.loc[k, 'bela_just_ner'][0]['entities']+df_rubq.loc[k, 'bela_just_ner_ru'][0]['entities']\n",
    "\n",
    "    new_arr =[ents[i] for i in inds]\n",
    "\n",
    "    cur_set = set()\n",
    "    fin_ents = []\n",
    "    for j in new_arr:\n",
    "        if j not in cur_set:\n",
    "            cur_set.add(j)\n",
    "            fin_ents.append(j)\n",
    "    df_rubq.loc[k, 'bela_just_ner_joined'] = ', '.join(fin_ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "3a62667e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 accuracy: 0.7668187001140251\n",
      "Top-2 accuracy: 0.8574686431014823\n",
      "Top-3 accuracy: 0.8797035347776511\n",
      "Top-4 accuracy: 0.8928164196123147\n",
      "Top-5 accuracy: 0.8973774230330672\n",
      "Top-6 accuracy: 0.9007981755986317\n"
     ]
    }
   ],
   "source": [
    "topk_accuracy(df_rubq, 'bela_base_joined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "4f528d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 accuracy: 0.7183580387685291\n",
      "Top-2 accuracy: 0.822690992018244\n",
      "Top-3 accuracy: 0.8517673888255416\n",
      "Top-4 accuracy: 0.8665906499429875\n",
      "Top-5 accuracy: 0.8734321550741163\n",
      "Top-6 accuracy: 0.878563283922463\n"
     ]
    }
   ],
   "source": [
    "topk_accuracy(df_rubq, 'bela_large_case_joined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "4d75003c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 accuracy: 0.7537058152793614\n",
      "Top-2 accuracy: 0.8472063854047891\n",
      "Top-3 accuracy: 0.8740022805017104\n",
      "Top-4 accuracy: 0.8825541619156214\n",
      "Top-5 accuracy: 0.8876852907639681\n",
      "Top-6 accuracy: 0.8916761687571265\n"
     ]
    }
   ],
   "source": [
    "topk_accuracy(df_rubq, 'bela_large_ner_joined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "4155fe3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 accuracy: 0.6470923603192702\n",
      "Top-2 accuracy: 0.7120866590649944\n",
      "Top-3 accuracy: 0.7200684150513113\n",
      "Top-4 accuracy: 0.7246294184720639\n",
      "Top-5 accuracy: 0.7274800456100342\n",
      "Top-6 accuracy: 0.7297605473204105\n"
     ]
    }
   ],
   "source": [
    "topk_accuracy(df_rubq, 'bela_just_ner_joined')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
